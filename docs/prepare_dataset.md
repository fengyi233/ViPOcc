
# ðŸ’¾ Datasets Preparation

**KITTI-360**

- Download perspective images, fisheye images, raw Velodyne scans, calibrations, and vehicle poses of [KITTI-360](https://www.cvlibs.net/datasets/kitti-360/index.php), unzip and save them to folder `data/KITTI-360`.


- Following [BTS](https://github.com/Brummi/BehindTheScenes) and [KYN](https://github.com/ruili3/Know-Your-Neighbors), we preprocess the images with the command below. It rectifies the fisheye views, resizes all images, and stores them
  in separate folders.

```bash
python data_preprocess/preprocess_kitti_360.py
```

- For efficient depth evaluation, we preprocess the velodyne_points in `data/KITTI-360/data_3d_raw/` directory, convert them into depth maps, and save them in `data/KITTI-360/depth_gt/`:

```bash
python data_preprocess/preprocess_kitti_360_data3draw.py [--save_png]
```

- For efficient training, we precompute **Pseudo Depths**
  from [Depth Anything V2](https://github.com/DepthAnything/Depth-Anything-V2), and save them in `data/KITTI-360/depth/`. Before doing this, you should download [official weights](https://huggingface.co/depth-anything/Depth-Anything-V2-Metric-VKITTI-Large/resolve/main/depth_anything_v2_metric_vkitti_vitl.pth?download=true), and place it in `data_preprocess/pseudo_depth/checkpoints`.

```bash
mkdir data_preprocess/pseudo_depth/checkpoints

wget -P data_preprocess/pseudo_depth/checkpoints https://huggingface.co/depth-anything/Depth-Anything-V2-Metric-VKITTI-Large/resolve/main/depth_anything_v2_metric_vkitti_vitl.pth

python data_preprocess/pseudo_depth/gen_pseudo_depth_kitti360.py
```

- (Optional) We precompute sampling anchors using visual priors from Grounded Segment Anything, and save them in `data/KITTI-360/samples/`. First, you should clone the repo [Grounded-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything) and set up the environment following their readme file. Then, put the scripts `data_preprocess/sampling_anchor/gen_sampling_anchors_kitti360.py` and `snog_sampler.py` in the root directory. Finally, run the following command: 

```bash
python gen_sampling_anchors_kitti360.py [--save_json] [--save_mask]
```
You can add `--save_json` and `--save_mask` to check the semantic output from Grounded-SAM. When the json files and semantic masks are saved, you can use the following command to visualize the sampling performance of SNOG sampler:
```bash
python data_preprocess/sampling_anchor/snog_sampler.py --save_dir 'visualization/sampling_anchors'
```
- We use precomputed ground truth from [KYN](https://github.com/ruili3/Know-Your-Neighbors). 
The data directory is set to `data/KITTI-360` by default. Download and unzip the pre-computed [GT occupancy maps](https://drive.google.com/file/d/17FvEShQdCRBSH91iQSMhcoocb8j9x3at/view?usp=drive_link) into `data/KITTI-360`. 
Download and unzip the [object labels](https://drive.google.com/file/d/1ELY2Hxy5hRP52J7ewzLYFWMk-Qu5QViQ/view?usp=drive_link) to `data/KITTI-360`.

The final folder structure should look like:

```bash
ViPOcc
  â””â”€â”€data
      â””â”€â”€KITTI-360
          â”œâ”€â”€ calibration
          â”œâ”€â”€ data_poses
          â”œâ”€â”€ data_2d_raw
          â”‚   â”œâ”€â”€ 2013_05_28_drive_0003_sync
          â”‚   â”‚   â”œâ”€â”€ image_00
          â”‚   â”‚   â”‚    â”œâ”€â”€ data_192x640
          â”‚   â”‚   â”‚    â””â”€â”€ data_rect
          â”‚   â”‚   â”œâ”€â”€ image_01
          â”‚   â”‚   â”œâ”€â”€ image_02
          â”‚   â”‚   â”‚    â”œâ”€â”€ data_192x640_0x-15
          â”‚   â”‚   â”‚    â””â”€â”€ data_rgb
          â”‚   â”‚   â””â”€â”€ image_03
          â”‚   â””â”€â”€ ...
          â”œâ”€â”€ data_3d_raw     # not used anymore
          â”‚   â”œâ”€â”€ 2013_05_28_drive_0003_sync
          â”‚   â””â”€â”€ ...
          â”œâ”€â”€ depth/          # Generated by Depth Anything V2
          â”œâ”€â”€ depth_gt/       # Generated from data_3d_raw
          â”œâ”€â”€ GT_Occ/         
          â”œâ”€â”€ Object_Label/   
          â””â”€â”€ samples/        # Generated by SNOG sampler, optional
```


